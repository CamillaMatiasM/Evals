import openai from "../openai-config.js";
import fs from "fs";

/**
 * Runs an evaluation using prompt, eval_ID and data_ID
 * @param {string} promptFilePath - Path to the prompt.txt file
 * @param {string} evalId - The OpenAI eval ID
 * @param {string} dataId - The OpenAI dataset ID
 * @returns {Promise<Object>} - The evaluation run result
 */
export async function run(promptFilePath, evalId, dataId, runConfigPath) {
  try {
    console.log("üöÄ Starting eval run...");
    console.log(`üìù Prompt file: ${promptFilePath}`);
    console.log(`üîç Eval ID: ${evalId}`);
    console.log(`üìä Data ID: ${dataId}`);

    // Read the prompt file
    if (!fs.existsSync(promptFilePath)) {
      throw new Error(`Prompt file not found: ${promptFilePath}`);
    }

    const prompt = fs.readFileSync(promptFilePath, "utf8").trim();
    console.log(`üìÑ Loaded prompt: ${prompt.substring(0, 100)}...`);

    const runConfig = JSON.parse(fs.readFileSync(runConfigPath, "utf8"));
    console.log(runConfig);

    // Create evaluation run using the platform API
    const evalRun = await openai.evals.runs.create(evalId, {
      ...runConfig,
      data_source: {
        type: runConfig.data_source.type,
        model: runConfig.data_source.model,
        input_messages: {
          type: runConfig.data_source.input_messages.type,
          template: [
            { role: "developer", content: prompt },
            { role: "user", content: "{{ item.input }}" },
          ],
        },
        source: { type: "file_id", id: dataId },
      },
    });

    console.log(`‚úÖ Eval run started with ID: ${evalRun.id}`);
    
    return evalRun;
  } catch (error) {
    console.error("‚ùå Error running eval:", error);
    throw error;
  }
}
